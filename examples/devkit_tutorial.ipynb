{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eb1c4ced",
   "metadata": {},
   "source": [
    "## Development-Kit Tutorial for Zenseact Open Dataset\n",
    "This notebook aims to introduce the ZodFrames & ZodSequences classes, which are helper classes to interact with the Frames and Sequences subsets of the Zenseact Open Dataset (ZOD) respecively. It will highlight some basic functionality that later can be used to build dataloaders in for example PyTorch.\n",
    "\n",
    "This notebook also aims to give a brief introduction to the which annotations exist and how to visualization them. \n",
    "\n",
    "#### The dataset includes data from 3 sensor modalities and calibrations for each sensor:  \n",
    "1. **Camera** - Anonymized (license plates and faces) front camera images. Available anonymization methods are:\n",
    "    - blur (Blur)\n",
    "    - dnat (Deep Fake)\n",
    "\n",
    "\n",
    "2. **LiDAR** - The LiDAR point cloud is the closest LiDAR scan to the camera timestamp of the core frame. Zenseact Open Dataset also provides a range of LiDAR point clouds captured in [-1s, +1s] at 10Hz around the core frame for the sequences.\n",
    "\n",
    "\n",
    "3. **OXTS** - High-precision GPS. OXTS data is provided in [-1s, ~10s] around the core frames for each sequence.\n",
    "\n",
    "#### There are 4 types of annotationed objects:  \n",
    "1. **dynamic_objects** - objects that can move (vehicles, pedestrians etc.) - annotated with 2D/3D bounding boxes\n",
    "2. **static_objects** - non-movable objects (light poles, traffic signs etc.) - annotated with 2D/3D bounding boxes\n",
    "3. **lane_markings** - lane markings and road paitings - annotated with polygons\n",
    "4. **ego_road** (Doesn't exist for all frames) - polygons that shows the road where ego vehicle can drive - annotated with polygons "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fa410652",
   "metadata": {},
   "source": [
    "# Initialization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c0069f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the ZOD DevKit\n",
    "from zod import ZodFrames\n",
    "from zod import ZodSequences\n",
    "\n",
    "# import default constants\n",
    "import zod.constants as constants\n",
    "from zod.constants import Camera, Lidar, Anonymization, AnnotationProject\n",
    "\n",
    "# set path to dataset and choose version\n",
    "data_dir = \"/staging/dataset_donation/round_2\"\n",
    "version = \"mini\"  # \"mini\" or \"full\"\n",
    "\n",
    "# initialize ZodFrames\n",
    "zod_frames = ZodFrames(dataset_root=data_dir, version=version)\n",
    "\n",
    "# initialize ZodSequences\n",
    "zod_sequences = ZodSequences(dataset_root=data_dir, version=version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7f8f88",
   "metadata": {},
   "source": [
    "### Split into Training and Validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525d2bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get default training and validation splits\n",
    "training_frames = zod_frames.get_split(constants.TRAIN)\n",
    "validation_frames = zod_frames.get_split(constants.VAL)\n",
    "\n",
    "# print the number of training and validation frames\n",
    "print(f\"Number of training frames: {len(training_frames)}\")\n",
    "print(f\"Number of validation frames: {len(validation_frames)}\")\n",
    "\n",
    "training_sequences = zod_sequences.get_split(constants.TRAIN)\n",
    "validation_sequences = zod_sequences.get_split(constants.VAL)\n",
    "print(f\"Number of training sequences: {len(training_sequences)}\")\n",
    "print(f\"Number of validation sequences: {len(validation_sequences)}\")\n",
    "\n",
    "# print out the first 5 training frames\n",
    "print(\"The 5 first training frames have the ids:\", training_frames[:5])\n",
    "\n",
    "# show the first training sequence\n",
    "print(\"The first training sequence has the id:\", training_sequences[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4b1f1943",
   "metadata": {},
   "source": [
    "### Fetch a ZodFrame\n",
    "The ZodFrames class yeild a `ZodFrame` which acts a cache for the light-weight data (e.g., ego-motion, calibration, and metadata), but also holds an `info` attribute. This in turn holds all the paths to more heavy-weight data (e.g., images and point clouds).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db74f119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can get a specific frame by its id\n",
    "frame_from_id = zod_frames[\"009158\"]\n",
    "# or via the index\n",
    "frame_from_idx = zod_frames[9158]\n",
    "\n",
    "# these two frames are the same\n",
    "assert frame_from_id.info == frame_from_idx.info"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4eea3e11",
   "metadata": {},
   "source": [
    "### Look at some data within a ZodFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27ac6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "zod_frame = zod_frames[62592]\n",
    "\n",
    "# we can access the metadata of a frame\n",
    "metadata = zod_frame.metadata\n",
    "\n",
    "# print a subsample of meta data\n",
    "print(f\"Frame id: {metadata.frame_id}\")\n",
    "print(f\"Country Code: {metadata.country_code}\")\n",
    "print(f\"Time of day: {metadata.time_of_day}\")\n",
    "print(f\"Number of vehicles in the frame: {metadata.num_vehicles}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244134dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can use the frame to get the ego-motion of our the vehicle\n",
    "ego_motion = zod_frame.ego_motion\n",
    "print(f\"Acceleration: {ego_motion.accelerations.shape}\")\n",
    "print(f\"Velocities: {ego_motion.velocities.shape}\")\n",
    "print(f\"Poses: {ego_motion.poses.shape}\")\n",
    "print(f\"Timestamps: {ego_motion.timestamps.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b3f727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that the ego-motion is a lightwieght version of the oxts data\n",
    "oxts = zod_frame.oxts\n",
    "print(f\"Acceleration: {oxts.accelerations.shape}\")\n",
    "print(f\"Velocities: {oxts.velocities.shape}\")\n",
    "print(f\"Poses: {oxts.poses.shape}\")\n",
    "print(f\"Timestamps: {oxts.timestamps.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93fb4e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can also get the calibrations\n",
    "calibrations = zod_frame.calibration\n",
    "\n",
    "print(calibrations.lidars[Lidar.VELODYNE])\n",
    "print(calibrations.cameras[Camera.FRONT])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bcfa8a52",
   "metadata": {},
   "source": [
    "#### Camera Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f500d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the camera core-frame from front camera with dnat anonymization\n",
    "camera_core_frame = zod_frame.info.get_key_camera_frame(Anonymization.DNAT)\n",
    "print(camera_core_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378aecc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = [20, 10]\n",
    "\n",
    "# one can read the image from the filepath\n",
    "image = camera_core_frame.read()\n",
    "# or use a helper directly from the frame\n",
    "zod_frame.get_image(Anonymization.DNAT)\n",
    "\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(image)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "00a57dde",
   "metadata": {},
   "source": [
    "#### LiDAR Data\n",
    "Lidar fields description:\n",
    "\n",
    "| Name | Type | Units | Description |\n",
    "| --- | --- | --- | --- |\n",
    "| 'timestamp' | string |  seconds  | UTC timestamp of each point. |\n",
    "| 'x' | double |  meters  | x coordinate of the point in lidar frame |\n",
    "| 'y' | double |  meters  | y coordinate of the point in lidar frame |\n",
    "| 'z' | double |  meters  | z coordinate of the point in lidar frame |\n",
    "| 'intensity' | double |    | intensity level of each point in range [0..255] |\n",
    "| 'diode_index' | integer |    | index of diode emitter which produced a point (1..128) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a119c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "zod_frame = zod_frames[62592]\n",
    "\n",
    "# get the lidar core-frame\n",
    "lidar_core_frame = zod_frame.info.get_key_lidar_frame()\n",
    "print(lidar_core_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0225fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the lidar data\n",
    "pc = lidar_core_frame.read()\n",
    "\n",
    "# This returns a zod LidarData dataclass, which is a wrapper around several numpy arrays\n",
    "from zod.zod_dataclasses import LidarData\n",
    "assert isinstance(pc, LidarData)\n",
    "\n",
    "# Alternatively, we can use helper functions on the frame itself\n",
    "assert zod_frame.get_lidar_data()[0] == pc\n",
    "assert zod_frame.get_lidar_frames()[0].read() == pc\n",
    "\n",
    "print(f\"Points: {pc.points.shape}\")  # x, y, z\n",
    "print(f\"Timestamps: {pc.timestamps.shape}\")\n",
    "print(f\"Intensity: {pc.intensity.shape}\")\n",
    "print(f\"Diode: {pc.diode_idx.shape}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9c27e33c",
   "metadata": {},
   "source": [
    "#### Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8920f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a new frame\n",
    "zod_frame = zod_frames[\"082291\"]\n",
    "\n",
    "# get the object annotations\n",
    "annotations = zod_frame.get_annotation(AnnotationProject.OBJECT_DETECTION)\n",
    "\n",
    "# get a single annotation object by index\n",
    "idx = 31\n",
    "print(f\"Annotation: {annotations[idx].name}\")\n",
    "\n",
    "# there are both 2d and 3d annotations\n",
    "annotation_2d = annotations[idx].box2d\n",
    "annotation_3d = annotations[idx].box3d\n",
    "print(annotation_2d)\n",
    "print(annotation_3d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3820f177",
   "metadata": {},
   "outputs": [],
   "source": [
    "from zod.visualization.object_visualization import overlay_object_2d_box_on_image\n",
    "from zod.visualization.object_visualization import overlay_object_3d_box_on_image\n",
    "\n",
    "# we can overlay the 2d annotation on the front camera image\n",
    "camera_core_frame = zod_frame.info.get_key_camera_frame(Anonymization.DNAT)\n",
    "image = plt.imread(camera_core_frame.filepath)\n",
    "\n",
    "image = overlay_object_2d_box_on_image(image, annotation_2d, color=(255, 0, 0), line_thickness=10)\n",
    "\n",
    "plt.figure()\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(image)\n",
    "\n",
    "# we can also overlay the 3d annotation on the front camera image,\n",
    "# but for this we also need the calibrations of the sensor\n",
    "calibrations = zod_frame.calibration\n",
    "\n",
    "# overlay the 3d box on the image\n",
    "image = overlay_object_3d_box_on_image(\n",
    "    image, annotation_3d, calibrations, color=(255, 0, 0), line_thickness=10\n",
    ")\n",
    "\n",
    "plt.figure()\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e12472",
   "metadata": {},
   "outputs": [],
   "source": [
    "from zod.frames.polygon_annotations.polygon_transformations import polygons_to_binary_mask\n",
    "\n",
    "zod_frame = zod_frames[9158]\n",
    "\n",
    "# get the ego road annotations\n",
    "polygon_annotations = zod_frame.get_annotation(AnnotationProject.EGO_ROAD)\n",
    "\n",
    "# convert the polygons to a binary mask (which can be used\n",
    "# for ground truth in e.g. semantic segmentation)\n",
    "mask = polygons_to_binary_mask(polygon_annotations)\n",
    "\n",
    "# visualize the mask\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e7ff11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get another frame\n",
    "zod_frame = zod_frames[23996]\n",
    "\n",
    "# get the lane markings annotations\n",
    "project = constants.AnnotationProject.LANE_MARKINGS\n",
    "polygon_annotations = zod_frame.get_annotation(project)\n",
    "\n",
    "# convert the polygons to a binary mask\n",
    "mask = polygons_to_binary_mask(polygon_annotations)\n",
    "\n",
    "# visualize the mask\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191b3bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can overlay the ego road annotations on the image\n",
    "from zod.visualization.polygon_utils import overlay_mask_on_image\n",
    "from zod.frames.polygon_annotations.polygon_transformations import polygons_to_binary_mask\n",
    "\n",
    "zod_frame = zod_frames[9158]\n",
    "\n",
    "# get the camera core-frame from front camera with dnat anonymization\n",
    "camera_core_frame = zod_frame.info.get_key_camera_frame(Anonymization.DNAT)\n",
    "\n",
    "# get the image\n",
    "image = plt.imread(camera_core_frame.filepath)\n",
    "\n",
    "# get the ego road annotations\n",
    "polygon_annotations = zod_frame.get_annotation(AnnotationProject.EGO_ROAD)\n",
    "\n",
    "# convert the polygons to a binary mask (which can be used\n",
    "# for ground truth in e.g. semantic segmentation)\n",
    "mask = polygons_to_binary_mask(polygon_annotations)\n",
    "\n",
    "# overlay the mask on the image\n",
    "image = overlay_mask_on_image(mask, image, fill_color=(100, 0, 0), alpha=0.5)\n",
    "\n",
    "# visualize the mask\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3f093e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can overlay the lane markings annotations on the image\n",
    "zod_frame = zod_frames[29229]\n",
    "\n",
    "# get the camera core-frame from front camera with dnat anonymization\n",
    "camera_core_frame = zod_frame.info.get_key_camera_frame(Anonymization.DNAT)\n",
    "\n",
    "# get the image\n",
    "fp = camera_core_frame.filepath.replace(\n",
    "    \"/staging/dataset_donation/round_2\", \"/Users/s0001621/data/zod\"\n",
    ")\n",
    "image = plt.imread(fp)\n",
    "\n",
    "# get the ego road annotations\n",
    "polygon_annotations = zod_frame.get_annotation(AnnotationProject.LANE_MARKINGS)\n",
    "\n",
    "# convert the polygons to a binary mask (which can be used\n",
    "# for ground truth in e.g. semantic segmentation)\n",
    "mask = polygons_to_binary_mask(polygon_annotations)\n",
    "\n",
    "# overlay the mask on the image\n",
    "image = overlay_mask_on_image(mask, image, fill_color=(0, 0, 100), alpha=0.75)\n",
    "\n",
    "# visualize the mask\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8d7764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize LiDAR and objects in Bird's Eye View\n",
    "from zod.visualization.lidar_bev import BEVBox\n",
    "from zod.zod_dataclasses import LidarData\n",
    "\n",
    "zod_frame = zod_frames[\"009158\"]\n",
    "\n",
    "# get the LiDAR point cloud\n",
    "pcd = zod_frame.get_lidar_data()[0]\n",
    "\n",
    "# get the object annotations\n",
    "object_annotations = zod_frame.get_annotation(AnnotationProject.OBJECT_DETECTION)\n",
    "\n",
    "import numpy as np\n",
    "bev = BEVBox()\n",
    "bev_image = bev(\n",
    "    np.hstack((pcd.points, pcd.intensity[:, None])),\n",
    "    (\n",
    "        np.array([obj.name for obj in object_annotations if obj.box3d]),\n",
    "        np.concatenate(\n",
    "            [obj.box3d.center[None, :] for obj in object_annotations if obj.box3d], axis=0\n",
    "        ),\n",
    "        np.concatenate(\n",
    "            [obj.box3d.size[None, :] for obj in object_annotations if obj.box3d], axis=0\n",
    "        ),\n",
    "        [obj.box3d.orientation for obj in object_annotations if obj.box3d],\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0723fb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can also visualize the lidar point cloud in the image\n",
    "from zod.visualization.lidar_on_image import visualize_lidar_on_image\n",
    "from zod.zod_dataclasses import LidarData\n",
    "\n",
    "zod_frame = zod_frames[\"087912\"]\n",
    "\n",
    "image = zod_frame.get_image()\n",
    "\n",
    "# Plot single Lidar point cloud\n",
    "core_lidar = zod_frame.get_lidar_data()[0]\n",
    "lid_image = visualize_lidar_on_image(\n",
    "    core_lidar, \n",
    "    zod_frame.calibration, \n",
    "    image,\n",
    ")\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(lid_image)\n",
    "plt.show()\n",
    "\n",
    "# Plot aggregated Lidar point cloud\n",
    "aggregated_lidar = zod_frame.get_aggregated_point_cloud(num_before=10, num_after=0)\n",
    "lid_image = visualize_lidar_on_image(\n",
    "    aggregated_lidar, \n",
    "    zod_frame.calibration, \n",
    "    image,\n",
    ")\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(lid_image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382a8eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can also visualize all together\n",
    "zod_frame = zod_frames[9158]\n",
    "\n",
    "pcd = zod_frame.get_aggregated_point_cloud(num_before=3)\n",
    "annotations = zod_frame.get_annotation(AnnotationProject.OBJECT_DETECTION)\n",
    "polygon_annotations = zod_frame.get_annotation(AnnotationProject.EGO_ROAD)\n",
    "mask = polygons_to_binary_mask(polygon_annotations)\n",
    "calibrations = zod_frame.calibration\n",
    "image = zod_frame.get_image(Anonymization.DNAT)\n",
    "\n",
    "# overlay the mask/annotation/pointcloud on the image\n",
    "image = visualize_lidar_on_image(pcd, calibrations, image)\n",
    "image = overlay_mask_on_image(mask, image, fill_color=(100, 0, 0), alpha=0.5)\n",
    "for annotation in annotations:\n",
    "    if annotation.box3d:\n",
    "        image = overlay_object_3d_box_on_image(\n",
    "            image, annotation.box3d, calibrations, color=(0, 100, 0), line_thickness=10\n",
    "        )\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "327cbef2",
   "metadata": {},
   "source": [
    "# END OF STUFF THAT ACTUALLY WORKS #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f176cf8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "raise NotImplementedError(\"now stuff breaks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720ce4aa",
   "metadata": {},
   "source": [
    "### Visualize OXTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2509bc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    from zod.visualization.oxts_visualization import plot_gps_track_from_dataset_sequence\n",
    "\n",
    "    frame_id = \"029229\"\n",
    "    ego_motion = zod.read_ego_motion(frame_id)\n",
    "\n",
    "    # plot GPS track on interactive map\n",
    "    plot_gps_track_from_dataset_sequence(ego_motion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa628f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Todo: fix this\n",
    "if False:\n",
    "    from zod.visualization.oxts_on_image import visualize_gps_on_image\n",
    "    import cv2\n",
    "\n",
    "    # visualize GPS track over image\n",
    "    timestamp = zod.get_timestamp(frame_id)\n",
    "    camera_calib = zod.read_calibration(frame_id).cameras[\"camera_front\"]\n",
    "\n",
    "    gps_on_image = visualize_gps_on_image(ego_motion, timestamp, camera_calib, image)\n",
    "    gps_on_image = cv2.cvtColor(gps_on_image, cv2.COLOR_BGR2RGB)\n",
    "    plt.imshow(gps_on_image)\n",
    "    plt.title(\"GPS on image\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9dd078c",
   "metadata": {},
   "source": [
    "#### Visualize all objects (both static and dynamic) in a scene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7df74ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from zod.visualization.object_visualization import (\n",
    "    overlay_object_2d_box_on_image,\n",
    "    overlay_object_properties_on_image,\n",
    ")\n",
    "\n",
    "frame_id = \"018591\"\n",
    "\n",
    "object_annotations = zod.read_object_detection_annotation(frame_id)\n",
    "\n",
    "image = plt.imread(zod.get_image_path(frame_id))\n",
    "\n",
    "text_areas = []\n",
    "for object_index, object_annotation in enumerate(object_annotations):\n",
    "    image = overlay_object_2d_box_on_image(image, object_annotation.box2d)\n",
    "    image = overlay_object_properties_on_image(\n",
    "        image, object_annotation, properties_list=[\"name\"], color=(255, 255, 0), text_areas=[]\n",
    "    )\n",
    "\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1e6dae",
   "metadata": {},
   "source": [
    "#### Visualize only vehicles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c2bfdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from zod.visualization.object_visualization import (\n",
    "    overlay_object_2d_box_on_image,\n",
    "    overlay_object_properties_on_image,\n",
    ")\n",
    "\n",
    "frame_id = \"018591\"\n",
    "\n",
    "object_annotations = zod.read_object_detection_annotation(frame_id)\n",
    "\n",
    "image = plt.imread(zod.get_image_path(frame_id))\n",
    "\n",
    "for object_index, object_annotation in enumerate(object_annotations):\n",
    "    text_areas = []\n",
    "    if object_annotation.name == \"Vehicle\":\n",
    "        image = overlay_object_2d_box_on_image(image, object_annotation.box2d)\n",
    "        image = overlay_object_properties_on_image(\n",
    "            image,\n",
    "            object_annotation,\n",
    "            properties_list=[\"object_id\"],\n",
    "            color=(255, 255, 0),\n",
    "            object_id=object_index,\n",
    "            text_areas=[],\n",
    "        )\n",
    "\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862dac1e",
   "metadata": {},
   "source": [
    "#### Visualize only pedestrians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e317fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from zod.visualization.object_visualization import (\n",
    "    overlay_object_2d_box_on_image,\n",
    "    overlay_object_properties_on_image,\n",
    ")\n",
    "\n",
    "frame_id = \"062592\"\n",
    "\n",
    "object_annotations = zod.read_object_detection_annotation(frame_id)\n",
    "\n",
    "image = plt.imread(zod.get_image_path(frame_id))\n",
    "\n",
    "for object_index, object_annotation in enumerate(object_annotations):\n",
    "    text_areas = []\n",
    "    if object_annotation.name == \"Pedestrian\":\n",
    "        image = overlay_object_2d_box_on_image(image, object_annotation.box2d)\n",
    "        image = overlay_object_properties_on_image(\n",
    "            image,\n",
    "            object_annotation,\n",
    "            properties_list=[\"object_id\"],\n",
    "            color=(255, 255, 0),\n",
    "            object_id=object_index,\n",
    "            text_areas=[],\n",
    "        )\n",
    "\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c948f7a",
   "metadata": {},
   "source": [
    "#### Visualize only pole objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358ad8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from zod.visualization.object_visualization import (\n",
    "    overlay_object_2d_box_on_image,\n",
    "    overlay_object_properties_on_image,\n",
    ")\n",
    "\n",
    "frame_id = \"009158\"\n",
    "\n",
    "object_annotations = zod.read_object_detection_annotation(frame_id)\n",
    "\n",
    "image = plt.imread(zod.get_image_path(frame_id))\n",
    "\n",
    "for object_index, object_annotation in enumerate(object_annotations):\n",
    "    text_areas = []\n",
    "    if object_annotation.name == \"PoleObject\":\n",
    "        image = overlay_object_2d_box_on_image(image, object_annotation.box2d)\n",
    "        image = overlay_object_properties_on_image(\n",
    "            image,\n",
    "            object_annotation,\n",
    "            properties_list=[\"object_type\"],\n",
    "            color=(255, 0, 0),\n",
    "            text_areas=[],\n",
    "        )\n",
    "\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210eb110",
   "metadata": {},
   "source": [
    "#### Visualize only Traffic Signs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bb62a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from zod.visualization.object_visualization import (\n",
    "    overlay_object_2d_box_on_image,\n",
    "    overlay_object_properties_on_image,\n",
    ")\n",
    "\n",
    "frame_id = \"062592\"\n",
    "\n",
    "object_annotations = zod.read_object_detection_annotation(frame_id)\n",
    "\n",
    "image = plt.imread(zod.get_image_path(frame_id))\n",
    "\n",
    "for object_index, object_annotation in enumerate(object_annotations):\n",
    "    text_areas = []\n",
    "    if object_annotation.name == \"TrafficSign\":\n",
    "        image = overlay_object_2d_box_on_image(image, object_annotation.box2d)\n",
    "        image = overlay_object_properties_on_image(\n",
    "            image,\n",
    "            object_annotation,\n",
    "            properties_list=[\"object_id\"],\n",
    "            color=(255, 0, 0),\n",
    "            text_areas=[],\n",
    "            object_id=object_index,\n",
    "        )\n",
    "\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12bcf24e",
   "metadata": {},
   "source": [
    "### Visualize lane marking annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652fb342",
   "metadata": {},
   "outputs": [],
   "source": [
    "from zod.visualization.lane_markings_visualization import overlay_lane_markings_on_image\n",
    "\n",
    "frame_id = \"029229\"\n",
    "lane_markings_annotation = zod.read_lane_markings_annotation(frame_id)\n",
    "image = plt.imread(zod.get_image_path(frame_id))\n",
    "image = overlay_lane_markings_on_image(lane_markings_annotation, image)\n",
    "\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cdc16dd",
   "metadata": {},
   "source": [
    "### Ego Road annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65006b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from zod.visualization.ego_road_visualization import overlay_ego_road_on_image\n",
    "\n",
    "frame_id = \"062592\"\n",
    "ego_road_annotation = zod.read_ego_road_annotation(frame_id)\n",
    "image = plt.imread(zod.get_image_path(frame_id))\n",
    "image = overlay_ego_road_on_image(ego_road_annotation, image)\n",
    "\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zodv3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "8d826bf553a17888f89a314b0090aa7e6bdb38e87fff2ac0e39c989009b56d89"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
